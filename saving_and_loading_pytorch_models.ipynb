{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "saving and loading pytorch models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNDn04LuULUP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "    self.pool1 = nn.MaxPool2d(2,2)\n",
        "    self.bn = nn.BatchNorm2d(16)\n",
        "    self.fc1 = nn.Linear(256,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.pool1(x)\n",
        "    x=self.bn(x)\n",
        "    x=self.Linear(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5luQ1_aV8gR"
      },
      "source": [
        "my_model = model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hh7xORYRCG"
      },
      "source": [
        "We can save either the model archeticture + weights or just the weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpchYL6lYkfe"
      },
      "source": [
        "##Saving the archeticture + weights:\n",
        "The model is saved as pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5eAPCpDVY56"
      },
      "source": [
        "model_path = 'models'\n",
        "torch.save(my_model , model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVvxyA58V5yx"
      },
      "source": [
        "#to load the model use torch.load()\n",
        "#no need to define the model archeticture (object) first\n",
        "new_model = torch.load(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4auol1pZMQ-"
      },
      "source": [
        "##Saving just the model weights (state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj_QZ5QeZKyG",
        "outputId": "061e5f19-277a-48cb-d96d-3bf833521f8e"
      },
      "source": [
        "#The state_dict is a python dictionary that contains all the layers paprametars + register buffer (like BN parameters)\n",
        "for param in my_model.state_dict():\n",
        "  print(f'{param} shape {my_model.state_dict()[param].shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.weight shape torch.Size([16, 3, 3, 3])\n",
            "conv1.bias shape torch.Size([16])\n",
            "bn.weight shape torch.Size([16])\n",
            "bn.bias shape torch.Size([16])\n",
            "bn.running_mean shape torch.Size([16])\n",
            "bn.running_var shape torch.Size([16])\n",
            "bn.num_batches_tracked shape torch.Size([])\n",
            "fc1.weight shape torch.Size([10, 256])\n",
            "fc1.bias shape torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCy8gHTqZmN0"
      },
      "source": [
        "params_path='model_state_dict'\n",
        "torch.save(my_model.state_dict() , params_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bNBtueba6W9",
        "outputId": "cacaad3f-7414-4e30-a9e1-5a1e8655cae7"
      },
      "source": [
        "#We can not use torch.load() directly to load the weights!. We need to define the model archeticure first.\n",
        "#If the model matches the weights it will work well\n",
        "my_new_model = model()\n",
        "state_dict = torch.load(params_path)\n",
        "my_new_model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LERWTILo4g_U"
      },
      "source": [
        "## Strict Loading:\n",
        "load_state_dict(state_dict, strict=True)\n",
        "\n",
        "If strict is True, then the keys of state_dict must exactly match the keys returned by this module’s state_dict() function.\n",
        "\n",
        "load_state_dict() returns:\n",
        "\n",
        "\n",
        "1.   **missing_keys** : a list of str containing the missing keys\n",
        "\n",
        "2.   **unexpected_keys** : a list of str containing the unexpected keys\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odw2xBrq5kx_"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50().cuda(gpu)\n",
        "state_dict = torch.load(args.pretrained, map_location='cpu')\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == []\n",
        "model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
        "model.fc.bias.data.zero_()\n",
        "if args.weights == 'freeze':\n",
        "    model.requires_grad_(False)\n",
        "    model.fc.requires_grad_(True)\n",
        "classifier_parameters, model_parameters = [], []\n",
        "for name, param in model.named_parameters():\n",
        "    if name in {'fc.weight', 'fc.bias'}:\n",
        "        classifier_parameters.append(param)\n",
        "    else:\n",
        "        model_parameters.append(param)\n",
        "\n",
        "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
        "\n",
        "param_groups = [dict(params=classifier_parameters, lr=args.lr_classifier)]\n",
        "if args.weights == 'finetune':\n",
        "    param_groups.append(dict(params=model_parameters, lr=args.lr_backbone))\n",
        "optimizer = optim.SGD(param_groups, 0, momentum=0.9, weight_decay=args.weight_decay)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MshFFW2FZRfp"
      },
      "source": [
        "from pathlib import Path\n",
        "p=Path('data')\n",
        "p.mkdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTBye-cgmWlW",
        "outputId": "83debb33-7b30-4746-be42-0f76bb6166a5"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.7/dist-packages (0.10)\n",
            "Collecting torch-xla==1.9\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9MB 73kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.8.1\n",
            "    Uninstalling torch-xla-1.8.1:\n",
            "      Successfully uninstalled torch-xla-1.8.1\n",
            "Successfully installed torch-xla-1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "u4YdT8vvnbzY",
        "outputId": "36bccf2f-3baf-40be-a1f5-8806c8791cbb"
      },
      "source": [
        "!pip uyninstall torch\n",
        "!pip install torch==1.9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch-1.9.0+cu102.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "\n",
            "\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Collecting torch==1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/a9/b3cea4a97ffabd6639e71608814dbd08081e202e8ac9580250273c0541ff/torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4MB 11kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.9.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-xRPi54mLg7",
        "outputId": "b0c5b2bd-94c2-4ab7-ae58-70775a5b8210"
      },
      "source": [
        "import torch_xla"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMiGF3gIl4DU",
        "outputId": "db8c5902-fd36-4620-b799-df8439b06d8b"
      },
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='xla', index=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkWugJizlrOq",
        "outputId": "cd56172f-1940-48ff-a466-a82ce112e38d"
      },
      "source": [
        "len(xm.get_xla_supported_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4dgO4fDcMxi"
      },
      "source": [
        "##Optimizers also have parameters that we can save:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59dquF3YbZs8",
        "outputId": "ccbdc465-5fc2-4a09-b090-42219566e3d9"
      },
      "source": [
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.003, betas=(0.9, 0.90), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "for param in optimizer.state_dict():\n",
        "  print(f'{param}    shape      {optimizer.state_dict()[param]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state    shape      {}\n",
            "param_groups    shape      [{'lr': 0.003, 'betas': (0.9, 0.9), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-UXTgavgWHz"
      },
      "source": [
        "**Saving the entire optimizer object**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5aN1tSjcvzg"
      },
      "source": [
        "optimizer_path = 'adam_optimizer'\n",
        "torch.save(optimizer , optimizer_path)\n",
        "optimizer = torch.load(optimizer_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUob8U6Hgdhp"
      },
      "source": [
        "**Saving the optimizer state_dict:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJvRYc5kgxRc"
      },
      "source": [
        "optimizer_state_dict_path=\"adam_optimizer_state_dict\"\n",
        "torch.save(optimizer.state_dict() , optimizer_state_dict_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqGRaWM3fEyV"
      },
      "source": [
        "new_optimizer = torch.optim.Adam(my_model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQvfFcHJf1Me",
        "outputId": "b5ab4462-6b0f-4a5e-89d8-de7b719ed347"
      },
      "source": [
        "torch.load(optimizer_state_dict_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'param_groups': [{'amsgrad': False,\n",
              "   'betas': (0.9, 0.9),\n",
              "   'eps': 1e-08,\n",
              "   'lr': 0.003,\n",
              "   'params': [0, 1, 2, 3, 4, 5],\n",
              "   'weight_decay': 0}],\n",
              " 'state': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwiMpnEpfqkk",
        "outputId": "8ba78e7a-9576-4ca2-da32-6d8e125be163"
      },
      "source": [
        "#load the optimizer state_dict\n",
        "new_optimizer.load_state_dict(torch.load(optimizer_state_dict_path))\n",
        "\n",
        "for param in new_optimizer.state_dict():\n",
        "  print(f'{param}    shape      {new_optimizer.state_dict()[param]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state    shape      {}\n",
            "param_groups    shape      [{'lr': 0.003, 'betas': (0.9, 0.9), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "302bCC77dnsc"
      },
      "source": [
        "##Torch.save can save any python object!:\n",
        "We can save all our work in one ckpt file (model, optimizer, loss, epochs, ... )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqtgswLLd_Lh"
      },
      "source": [
        "path_for_all = 'all'\n",
        "epoch=8\n",
        "loss=0.022\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': my_model,\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            \n",
        "            }, path_for_all)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqESofFJeRY9"
      },
      "source": [
        "checkpoint = torch.load(path_for_all)\n",
        "model_to_continue_training = checkpoint['model']\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch=checkpoint['epoch']\n",
        "loss=checkpoint['loss']\n",
        "\n",
        "#continue training....\n",
        "#evaluate............."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0add-hDYlA6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caececbf-eace-4f1f-f45c-cac9c1dd7440"
      },
      "source": [
        "for param in model_to_continue_training.state_dict():\n",
        "  print(f'{param}    shape      {model_to_continue_training.state_dict()[param].shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.weight    shape      torch.Size([16, 3, 3, 3])\n",
            "conv1.bias    shape      torch.Size([16])\n",
            "bn.weight    shape      torch.Size([16])\n",
            "bn.bias    shape      torch.Size([16])\n",
            "bn.running_mean    shape      torch.Size([16])\n",
            "bn.running_var    shape      torch.Size([16])\n",
            "bn.num_batches_tracked    shape      torch.Size([])\n",
            "fc1.weight    shape      torch.Size([10, 256])\n",
            "fc1.bias    shape      torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLuZ68DQj-yy",
        "outputId": "4846baf9-c721-4073-88c0-cde95f3ac4d2"
      },
      "source": [
        "for param in optimizer.state_dict():\n",
        "  print(f'{param}    shape      {optimizer.state_dict()[param]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state    shape      {}\n",
            "param_groups    shape      [{'lr': 0.003, 'betas': (0.9, 0.9), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4pFEkB-kc5P",
        "outputId": "3b6c1ba4-2824-41e1-e7eb-13f0fe997993"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4VsStOk8s-",
        "outputId": "84ab92aa-b002-444e-c358-086f118ed89b"
      },
      "source": [
        "epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxgeqBhmk--C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mzJ8bnXnt5K"
      },
      "source": [
        "#Saving and laoding with GPU:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwidvsyGpgN8"
      },
      "source": [
        "##Saving and loading on GPU:\n",
        "If you trained your model on GPU and want to load it on GPU remember to use model.to(device='cuda') after loading the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGVzRiWLnxm3"
      },
      "source": [
        "device= torch.device(\"cuda\") #suppose the GPU is available\n",
        "model.to(device) #move the model to GPU\n",
        "torch.save(my_model , 'saved_model') #save the model on GPU . torch.save() saves the model on the same device\n",
        "model_cont = torch.load('saved_model')\n",
        "#it is important to move the model to GPU, even if the model was saved on GPU  \n",
        "model.to(device)\n",
        "##\n",
        "\n",
        "#Continue training on GPU\n",
        "\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvRI5AAXpqCY"
      },
      "source": [
        "### Saving and loading on different devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to9W6iOSp70g"
      },
      "source": [
        "#if you save the model on GPU and want to load it to CPU, use map_location=cpu_device\n",
        "cpu_device = troch.device('cpu')\n",
        "model = torch.load(path , map_location=cpu_device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWs6ULjEqKoA"
      },
      "source": [
        "#if you save the model on CPU and want to load it to GPU, use map_location=gpu_device\n",
        "\n",
        "gpu_device = troch.device('gpu')\n",
        "model = torch.load(path , map_location=gpu_device)\n",
        "model.to(gpu_device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsnUeC2sJjr"
      },
      "source": [
        "#Saving and Loading XLA Models:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfNqt6IsO17"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "path = 'xla_model_state_dict'\n",
        "xm.save(xla_model.state_dict(), path , mater_only=True , global_only=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5NnEHovsc2l"
      },
      "source": [
        "state_dict = torch.load('xla_model_state_dict', map_location='cpu')\n",
        "model.load_state_dict(state_dict)\n",
        "device=xm.xla_device()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfjk7xCRaAEi"
      },
      "source": [
        "#Strict Loadging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFdR2f31aOSm"
      },
      "source": [
        "resnet = trochvision.models.resnet50()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lceze-qOaNF-"
      },
      "source": [
        "# save only the conv layers\n",
        "torch.save(list(resnet.children())[0].state_dict(), 'resnet50_conv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLLqukBDaDCa"
      },
      "source": [
        "model = trochvision.models.resnet50()\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "#missing_keys [fc.weights , fc.bias]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}